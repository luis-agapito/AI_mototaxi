{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DataLoaders: Pipeline for feeding the training data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232c9825844d906a"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:56:24.792662Z",
     "start_time": "2024-01-04T15:56:24.732333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import mototaxi_utils as mutils\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f'Pytorch version {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the images into three datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f742ddd0c40162"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training data is organized in two sets: images **with** and **without** mototaxis. They are stored in the following directory structure.\n",
    "```\n",
    " mototaxi_training_images/\n",
    " ├── mototaxi/ (558 images)\n",
    " └── no_mototaxi/ (558 images)\n",
    "```\n",
    "Our goal is to split all the collected images (1116) into three mutually exclusive datasets: train, validation (aka. development), and test. \n",
    "\n",
    "This can be done manually calling method `torchvision.datasets.ImageFolder()` three times, one for each separate folder containing images for training, validation and test datasets. However, since we want to keep all our images in a single folder (not having to manually store them into three different folders), we use `torchvision.utils.data.random_split()` to automatically split the contents of `mototaxi_training_images` into three different datasets (70%, 20%, and 10% for the train, validation, and test datasets). The seed of the generator is fixed with `manual_seed()` so that the random sampling is kept reproducible for debugging purposes. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7ceb13fcf5f1019"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Number of images=782, type <class 'mototaxi_utils.CustomSubset'>\n",
      "val_dataset: Number of images=223, type <class 'mototaxi_utils.CustomSubset'>\n",
      "test_dataset: Number of images=111, type <class 'mototaxi_utils.CustomSubset'>\n",
      "{'mototaxi': 0, 'no_mototaxi': 1}\n"
     ]
    }
   ],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.405]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "img_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "    ])\n",
    "}\n",
    "\n",
    "img_dir = '~/Downloads/dldata/mototaxi_training_images'\n",
    "img_dataset = torchvision.datasets.ImageFolder(root=img_dir, transform=img_transforms[\"train\"])\n",
    "train_dataset, val_dataset, test_dataset = mutils.custom_random_split(img_dataset,\n",
    "                                                                      (0.7, 0.2, 0.1),\n",
    "                                                                      generator=torch.Generator().manual_seed(42)\n",
    "                                                                       )\n",
    "print(f'train_dataset: Number of images={len(train_dataset)}, type {type(train_dataset)}')\n",
    "print(f'val_dataset: Number of images={len(val_dataset)}, type {type(val_dataset)}')\n",
    "print(f'test_dataset: Number of images={len(test_dataset)}, type {type(test_dataset)}')\n",
    "\n",
    "print(img_dataset.class_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T01:14:54.063366Z",
     "start_time": "2024-01-06T01:14:54.006628Z"
    }
   },
   "id": "c37699c7c5ac95c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The three datasets are of type `Subset`, more specifically, of custom type `CustomSubset`, as discussed below. These dataset objects are used to create DataLoaders. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac0be007be9d45b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the DataLoader\n",
    "\n",
    "It's useful to perform a dry-run of the training process for taking a look at the actual images (indices) composing each minibatch of each training epoch, as printed in the output cell below. Pytorch methods don't allow returning these indices alongside the images comprising each minibatch. Thus, we customized two methods (look at `mototaxi_utils.py`) to enable such feature:\n",
    "- Method `custom_random_splits()` based on `torch.utils.data.random_splits()`\n",
    "- Class `CustomSubset()` based on  `torch.utils.data.Subset()`\n",
    "\n",
    "These customizations allow returning indices of the selected images, as seen in line \n",
    "> `inputs_and_y, data_indices = minibatch_data` ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878249e8fa6e6129"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "num_samples_per_epoch = 25 \n",
    "batch_size = 10 \n",
    "num_epochs = 4 \n",
    "num_workers = 0\n",
    "test_random_sampler = torch.utils.data.RandomSampler(data_source=test_dataset,\n",
    "                                                     replacement=False,\n",
    "                                                     num_samples=num_samples_per_epoch,\n",
    "                                                     generator=torch.Generator().manual_seed(42)\n",
    "                                                     )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_workers,\n",
    "                                          sampler=test_random_sampler\n",
    "                                          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:56:24.913091Z",
     "start_time": "2024-01-04T15:56:24.834490Z"
    }
   },
   "id": "6a76e07f064cc7eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspecting the DataLoader: Training dry-run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fecc7eba78ebcfa"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ------------------\n",
      "#0: [448, 176, 175, 962, 1008, 1111, 729, 63, 749, 1065]\n",
      "#1: [167, 995, 606, 367, 663, 1109, 708, 703, 548, 66]\n",
      "#2: [1071, 776, 115, 965, 846]\n",
      "\n",
      "Epoch 1 ------------------\n",
      "#0: [277, 82, 392, 448, 1065, 1058, 435, 1046, 982, 808]\n",
      "#1: [184, 776, 587, 66, 796, 516, 802, 54, 292, 426]\n",
      "#2: [167, 846, 713, 63, 94]\n",
      "\n",
      "Epoch 2 ------------------\n",
      "#0: [184, 817, 516, 868, 708, 384, 167, 606, 75, 331]\n",
      "#1: [91, 965, 661, 277, 990, 1033, 55, 237, 995, 799]\n",
      "#2: [802, 63, 917, 683, 776]\n",
      "\n",
      "Epoch 3 ------------------\n",
      "#0: [1071, 808, 990, 82, 779, 943, 482, 422, 182, 1111]\n",
      "#1: [1109, 817, 645, 796, 907, 348, 984, 95, 94, 485]\n",
      "#2: [1097, 460, 237, 184, 963]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch} ------------------')\n",
    "    for minibatch_index, minibatch_data in enumerate(test_loader):\n",
    "        inputs_and_y, data_indices = minibatch_data\n",
    "        inputs, y = inputs_and_y\n",
    "        print(f'#{minibatch_index}:', data_indices.tolist())\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:56:30.112889Z",
     "start_time": "2024-01-04T15:56:24.846773Z"
    }
   },
   "id": "62e6ceacefe6fb09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the output cell above we can verify that\n",
    "- Each epoch uses only 25 images instead of the default behavior of using the entire 111 images of the test dataset. Thus, using `num_samples_per_epoch` gives the user finer control for managing TAT.\n",
    "- There are no image repetitions between different batches within the same epoch.\n",
    "- The images are fairly different between epochs, although there could be some random repetitions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21059891541c1b1a"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:56:30.134249Z",
     "start_time": "2024-01-04T15:56:30.110954Z"
    }
   },
   "id": "7b774697111e3b3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
